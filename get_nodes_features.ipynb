{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407501ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from math_support import graph_random_walk, convert_sequence_to_graph, compute_index_subsample, graph_random_walk_fixed_start\n",
    "from data_loader import read_data, perform_ttv_split\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a639319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def extract_features_for_classifier(nnet, xembed_numpy, sembed_numpy, ylabel_numpy, eglist, \n",
    "                                    idx_train, idx_valid, idx_test, \n",
    "                                    nbatch=64):\n",
    "    \"\"\"\n",
    "    Извлекает признаки для каждой вершины после message passing и разделяет их на train/val/test\n",
    "    Запускает random walk из каждой вершины, гарантируя включение всех вершин\n",
    "    \n",
    "    Параметры:\n",
    "    - nnet: Обученная модель Edge_Sheaf_NNet\n",
    "    - xembed_numpy: Исходные признаки вершин (numpy array)\n",
    "    - ylabel_numpy: Метки классов вершин (numpy array)\n",
    "    - eglist: Список рёбер графа\n",
    "    - idx_train: Индексы вершин обучающей выборки\n",
    "    - idx_valid: Индексы вершин валидационной выборки\n",
    "    - idx_test: Индексы вершин тестовой выборки\n",
    "    - nbatch: Размер батча для random walk\n",
    "    \n",
    "    Возвращает:\n",
    "    - features_dict: Словарь с разделенными признаками и метками\n",
    "    \"\"\"\n",
    "    # Переводим модель в режим оценки\n",
    "    nnet.eval()\n",
    "    \n",
    "    # Конвертируем в тензор, если нужно\n",
    "    if isinstance(xembed_numpy, np.ndarray):\n",
    "        xembed = torch.from_numpy(xembed_numpy).double()\n",
    "    else:\n",
    "        xembed = xembed_numpy.clone()\n",
    "\n",
    "    if isinstance(sembed_numpy, np.ndarray):\n",
    "        sembed = torch.from_numpy(sembed_numpy).double()\n",
    "    else:\n",
    "        sembed = sembed_numpy.clone()\n",
    "    \n",
    "    # Количество вершин\n",
    "    num_vertices = sembed.shape[0]\n",
    "    \n",
    "    # Тензоры для агрегации признаков\n",
    "    final_features = torch.zeros_like(xembed)\n",
    "    feature_counts = torch.zeros(num_vertices)\n",
    "    \n",
    "    # Запускаем random walk из каждой вершины\n",
    "    for start_vertex in tqdm(list(range(num_vertices))):\n",
    "        # print(f'Processing random walk starting from vertex {start_vertex+1}/{num_vertices}')\n",
    "        \n",
    "        # Генерируем random walk, начиная с указанной вершины\n",
    "        random_walk_data = graph_random_walk_fixed_start(eglist, nbatch, start_vertex)\n",
    "        \n",
    "        # Конвертируем в представление графа\n",
    "        wgraph_numpy, idx_node = convert_sequence_to_graph(random_walk_data)\n",
    "        wgraph = torch.from_numpy(wgraph_numpy).double()\n",
    "        \n",
    "        # Получаем индексы для train/valid/test из текущего подграфа\n",
    "        idx_subsample_train = compute_index_subsample(idx_node, idx_train)\n",
    "        idx_subsample_valid = compute_index_subsample(idx_node, idx_valid)\n",
    "        idx_subsample_test = compute_index_subsample(idx_node, idx_test)\n",
    "        \n",
    "        # Выполняем message passing для этого подграфа (для всех вершин)\n",
    "        with torch.no_grad():\n",
    "            subgraph_features = perform_message_passing(nnet, xembed, sembed, wgraph, idx_node)\n",
    "        \n",
    "        # Обновляем признаки для всех типов вершин в текущем подграфе\n",
    "        for subset_name, idx_subset in [\n",
    "            ('train', idx_subsample_train), \n",
    "            ('valid', idx_subsample_valid), \n",
    "            ('test', idx_subsample_test)\n",
    "        ]:\n",
    "            # Если есть вершины текущего типа в подграфе\n",
    "            if len(idx_subset) > 0:\n",
    "                # Для каждой вершины в текущем подмножестве\n",
    "                for local_idx in idx_subset:\n",
    "                    # Получаем глобальный индекс вершины\n",
    "                    global_idx = idx_node[local_idx]\n",
    "                    \n",
    "                    # Обновляем признаки и счетчик\n",
    "                    final_features[global_idx] += subgraph_features[local_idx]\n",
    "                    feature_counts[global_idx] += 1\n",
    "    \n",
    "    # Усредняем признаки для каждой вершины\n",
    "    for i in range(num_vertices):\n",
    "        if feature_counts[i] > 0:\n",
    "            final_features[i] /= feature_counts[i]\n",
    "        else:\n",
    "            # Оставляем исходные признаки, если вершина не была включена ни в один random walk\n",
    "            final_features[i] = sembed[i]\n",
    "            print(f\"Warning: Vertex {i} not included in any random walk. Using original features.\")\n",
    "    \n",
    "    # Конвертируем обратно в numpy для использования с классификаторами\n",
    "    final_features_np = final_features.detach().cpu().numpy()\n",
    "    \n",
    "    # Разделяем данные на train/val/test\n",
    "    features_dict = {\n",
    "        'features': final_features_np,  # Полный набор признаков для всех вершин\n",
    "        'labels': ylabel_numpy,         # Полный набор меток для всех вершин\n",
    "        \n",
    "        # Разделенные наборы\n",
    "        'train_features': final_features_np[idx_train],\n",
    "        'train_labels': ylabel_numpy[idx_train],\n",
    "        \n",
    "        'val_features': final_features_np[idx_valid],\n",
    "        'val_labels': ylabel_numpy[idx_valid],\n",
    "        \n",
    "        'test_features': final_features_np[idx_test],\n",
    "        'test_labels': ylabel_numpy[idx_test],\n",
    "        \n",
    "        # Индексы для справки\n",
    "        'idx_train': idx_train,\n",
    "        'idx_valid': idx_valid,\n",
    "        'idx_test': idx_test\n",
    "    }\n",
    "    \n",
    "    print(f\"Features extracted successfully\")\n",
    "    print(f\"Train set: {len(idx_train)} samples\")\n",
    "    print(f\"Validation set: {len(idx_valid)} samples\")\n",
    "    print(f\"Test set: {len(idx_test)} samples\")\n",
    "    \n",
    "    return features_dict\n",
    "\n",
    "def perform_message_passing(nnet, xembed, sembed, wgraph, idx_node):\n",
    "    \"\"\"\n",
    "    Выполняет message passing для преобразования признаков\n",
    "    \n",
    "    Параметры:\n",
    "    - nnet: Обученная модель Edge_Sheaf_NNet\n",
    "    - xembed: Признаки всех вершин\n",
    "    - wgraph: Матрица смежности для подграфа\n",
    "    - idx_node: Индексы вершин в подграфе\n",
    "    \n",
    "    Возвращает:\n",
    "    - Преобразованные признаки после message passing\n",
    "    \"\"\"\n",
    "    # Инициализируем признаки вершин для message passing\n",
    "    xmaped = sembed[idx_node].clone()\n",
    "    \n",
    "    # Количество вершин в подграфе\n",
    "    num_subgraph_vertices = len(idx_node)\n",
    "    \n",
    "    # Сохраняем рёбра для эффективной обработки\n",
    "    edge_indices = []\n",
    "    for i in range(num_subgraph_vertices):\n",
    "        for j in range(num_subgraph_vertices):\n",
    "            if wgraph[i, j] > 0:\n",
    "                edge_indices.append((i, j))\n",
    "    \n",
    "    # Итерации message passing\n",
    "    for conv_idx in range(nnet.nconv):\n",
    "        if len(edge_indices) > 0:\n",
    "            # Подготавливаем признаки источников и целей\n",
    "            source_indices = [i for i, j in edge_indices]\n",
    "            target_indices = [j for i, j in edge_indices]\n",
    "            \n",
    "            source_features = sembed[source_indices]\n",
    "            target_features = sembed[target_indices]\n",
    "            \n",
    "            # Вычисляем все матрицы рёбер одним батчем\n",
    "            # print('features shapes', source_features.shape, target_features.shape)\n",
    "            edge_matrices_batch = nnet.get_edge_matrix(source_features, target_features)\n",
    "            # print(\"edge_matrices_batch\", edge_matrices_batch.shape)\n",
    "            \n",
    "            # Инициализируем для message passing\n",
    "            new_xmaped = torch.zeros_like(xembed)\n",
    "            node_counts = torch.zeros(num_subgraph_vertices).to(xmaped.device)\n",
    "            \n",
    "            # Применяем матрицы рёбер к признакам источников\n",
    "            messages = torch.bmm(\n",
    "                edge_matrices_batch,\n",
    "                xembed[source_indices].unsqueeze(2)\n",
    "            ).squeeze(2)\n",
    "            \n",
    "            # Накапливаем сообщения\n",
    "            for idx, (i, j) in enumerate(edge_indices):\n",
    "                weight = wgraph[i, j]\n",
    "                new_xmaped[j:j+1, :] += weight * messages[idx:idx+1]\n",
    "                node_counts[j] += weight\n",
    "            \n",
    "            # Нормализуем по общему весу\n",
    "            for j in range(num_subgraph_vertices):\n",
    "                if node_counts[j] > 0:\n",
    "                    new_xmaped[j] /= node_counts[j]\n",
    "            \n",
    "            # Обновляем признаки вершин\n",
    "            xmaped = new_xmaped\n",
    "        else:\n",
    "            # Если ребер нет, признаки не изменяются\n",
    "            pass\n",
    "    \n",
    "    return xmaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a560cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_user_item_embedding\n",
      "ylabel.shape = (19717,)\n"
     ]
    }
   ],
   "source": [
    "xembed, eglist, ylabel, ylprob, xsvd = read_data(embedding_dimension=1,\n",
    "                                                 dataset_name='PubMed', eps=1.0e-6)\n",
    "print('ylabel.shape = ' + str(ylabel.shape))\n",
    "nsample = xembed.shape[0]\n",
    "idx_train, idx_ttest, idx_valid = perform_ttv_split(nsample, ftrain=0.6, fttest=0.2, fvalid=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2275fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_dimension 64\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411c2db5696347459ca4e793f5ec371a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 128\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eca3f16897f405b81692aca6a28327b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 16\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece689930a17441c8b8f64b74051c6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 128\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d459b824db56451c85828f60a10e5c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 16\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cde8df142146828744ca8fafe18262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 128\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516e27df3ea9479588f9fb2f97139daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 64\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e4213c118b4b2fb65899ac130af808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 128\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab3881e2bf547178e930ae49cc30e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 64\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ad64b0970b4f80ba3eee96295cd4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 64\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8081823c744122977af843b08f2bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 16\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a077e94db4f8415192543ebd2acd6ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n",
      "embedding_dimension 16\n",
      "compute_user_item_embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17907bc7e449429fb8234912ee26ae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for filename in os.listdir(\"/home/ubuntu/simulations_my_svd/nnet_folder\"):\n",
    "    if '_PubMed_' not in filename:\n",
    "        continue\n",
    "\n",
    "    model_path = f\"/home/ubuntu/simulations_my_svd/nnet_folder/{filename}\"\n",
    "\n",
    "    model = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "    dataset_name = filename.split('_')[2]\n",
    "    embedding_dimension = int(re.findall(r\"dime_(\\d+)\", filename)[0])\n",
    "\n",
    "    if embedding_dimension <= 1:\n",
    "        continue\n",
    "\n",
    "    print(\"embedding_dimension\", embedding_dimension)\n",
    "    xembed, eglist, ylabel, ylprob, xsvd = read_data(embedding_dimension=embedding_dimension,\n",
    "                                                             dataset_name=dataset_name, eps=1.0e-6)\n",
    "\n",
    "    extracted_features_result = extract_features_for_classifier(model, xembed, xsvd, ylabel, eglist, \n",
    "                                        idx_train, idx_valid, idx_ttest, \n",
    "                                        nbatch=64)\n",
    "\n",
    "    folder_name = filename.split('.')[0]\n",
    "    os.makedirs(f\"/home/ubuntu/simulations_my_svd/classificator_features/{folder_name}\", exist_ok=True)\n",
    "\n",
    "    for key, arr in extracted_features_result.items():\n",
    "        np.save(f\"/home/ubuntu/simulations_my_svd/classificator_features/{folder_name}/{key}\", arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b12a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b8d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-sparse\n",
      "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-scatter\n",
      "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ubuntu/miniconda3/envs/graph_ml/lib/python3.11/site-packages (from torch-sparse) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /home/ubuntu/miniconda3/envs/graph_ml/lib/python3.11/site-packages (from scipy->torch-sparse) (1.24.3)\n",
      "Building wheels for collected packages: torch-sparse, torch-scatter\n",
      "\u001b[33m  DEPRECATION: Building 'torch-sparse' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'torch-sparse'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for torch-sparse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=1065968 sha256=5d4a91b2b9d42d43ba19e0ca0749d122b78d06d2ec999da21e08cf5227308216\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
      "\u001b[33m  DEPRECATION: Building 'torch-scatter' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'torch-scatter'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=503167 sha256=0aefd6aafdeff13a5026af53ae15c91fd2e8ec26b7340925a625461e950f0659\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
      "Successfully built torch-sparse torch-scatter\n",
      "Installing collected packages: torch-scatter, torch-sparse\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch-sparse]\n",
      "\u001b[1A\u001b[2KSuccessfully installed torch-scatter-2.1.2 torch-sparse-0.6.18\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-sparse torch-scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80602f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
