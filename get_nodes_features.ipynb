{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407501ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/main/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from math_support import graph_random_walk, convert_sequence_to_graph, compute_index_subsample, graph_random_walk_fixed_start\n",
    "from data_loader import read_data, perform_ttv_split\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def extract_features_for_classifier(nnet, xembed_numpy, ylabel_numpy, eglist, \n",
    "                                    idx_train, idx_valid, idx_test, \n",
    "                                    nbatch=64):\n",
    "    nnet.eval()\n",
    "    \n",
    "    if isinstance(xembed_numpy, np.ndarray):\n",
    "        xembed = torch.from_numpy(xembed_numpy).double()\n",
    "    else:\n",
    "        xembed = xembed_numpy.clone()\n",
    "    \n",
    "    num_vertices = xembed.shape[0]\n",
    "    \n",
    "    final_features = torch.zeros_like(xembed)\n",
    "    feature_counts = torch.zeros(num_vertices)\n",
    "    \n",
    "    for start_vertex in tqdm(list(range(num_vertices))):\n",
    "        \n",
    "        random_walk_data = graph_random_walk_fixed_start(eglist, nbatch, start_vertex)\n",
    "        \n",
    "        wgraph_numpy, idx_node = convert_sequence_to_graph(random_walk_data)\n",
    "        wgraph = torch.from_numpy(wgraph_numpy).double()\n",
    "        \n",
    "        idx_subsample_train = compute_index_subsample(idx_node, idx_train)\n",
    "        idx_subsample_valid = compute_index_subsample(idx_node, idx_valid)\n",
    "        idx_subsample_test = compute_index_subsample(idx_node, idx_test)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            subgraph_features = perform_message_passing(nnet, xembed, wgraph, idx_node)\n",
    "        \n",
    "        for subset_name, idx_subset in [\n",
    "            ('train', idx_subsample_train), \n",
    "            ('valid', idx_subsample_valid), \n",
    "            ('test', idx_subsample_test)\n",
    "        ]:\n",
    "            if len(idx_subset) > 0:\n",
    "                for local_idx in idx_subset:\n",
    "                    global_idx = idx_node[local_idx]\n",
    "                    \n",
    "                    final_features[global_idx] += subgraph_features[local_idx]\n",
    "                    feature_counts[global_idx] += 1\n",
    "    \n",
    "    for i in range(num_vertices):\n",
    "        if feature_counts[i] > 0:\n",
    "            final_features[i] /= feature_counts[i]\n",
    "        else:\n",
    "            final_features[i] = xembed[i]\n",
    "            print(f\"Warning: Vertex {i} not included in any random walk. Using original features.\")\n",
    "    \n",
    "    final_features_np = final_features.detach().cpu().numpy()\n",
    "    \n",
    "    features_dict = {\n",
    "        'features': final_features_np,\n",
    "        'labels': ylabel_numpy,\n",
    "        \n",
    "        'train_features': final_features_np[idx_train],\n",
    "        'train_labels': ylabel_numpy[idx_train],\n",
    "        \n",
    "        'val_features': final_features_np[idx_valid],\n",
    "        'val_labels': ylabel_numpy[idx_valid],\n",
    "        \n",
    "        'test_features': final_features_np[idx_test],\n",
    "        'test_labels': ylabel_numpy[idx_test],\n",
    "        \n",
    "        'idx_train': idx_train,\n",
    "        'idx_valid': idx_valid,\n",
    "        'idx_test': idx_test\n",
    "    }\n",
    "    \n",
    "    print(f\"Features extracted successfully\")\n",
    "    print(f\"Train set: {len(idx_train)} samples\")\n",
    "    print(f\"Validation set: {len(idx_valid)} samples\")\n",
    "    print(f\"Test set: {len(idx_test)} samples\")\n",
    "    \n",
    "    return features_dict\n",
    "\n",
    "def perform_message_passing(nnet, xembed, wgraph, idx_node):\n",
    "    xmaped = xembed[idx_node].clone()\n",
    "    \n",
    "    num_subgraph_vertices = len(idx_node)\n",
    "    \n",
    "    edge_indices = []\n",
    "    for i in range(num_subgraph_vertices):\n",
    "        for j in range(num_subgraph_vertices):\n",
    "            if wgraph[i, j] > 0:\n",
    "                edge_indices.append((i, j))\n",
    "    \n",
    "    # Итерации message passing\n",
    "    for conv_idx in range(nnet.nconv):\n",
    "        if len(edge_indices) > 0:\n",
    "            source_indices = [i for i, j in edge_indices]\n",
    "            target_indices = [j for i, j in edge_indices]\n",
    "            \n",
    "            source_features = xmaped[source_indices]\n",
    "            target_features = xmaped[target_indices]\n",
    "            \n",
    "            edge_matrices_batch = nnet.get_edge_matrix(source_features, target_features)\n",
    "            \n",
    "            new_xmaped = torch.zeros_like(xmaped)\n",
    "            node_counts = torch.zeros(num_subgraph_vertices).to(xmaped.device)\n",
    "            \n",
    "            messages = torch.bmm(\n",
    "                edge_matrices_batch,\n",
    "                source_features.unsqueeze(2)\n",
    "            ).squeeze(2)\n",
    "            \n",
    "            for idx, (i, j) in enumerate(edge_indices):\n",
    "                weight = wgraph[i, j]\n",
    "                new_xmaped[j:j+1, :] += weight * messages[idx:idx+1]\n",
    "                node_counts[j] += weight\n",
    "            \n",
    "            for j in range(num_subgraph_vertices):\n",
    "                if node_counts[j] > 0:\n",
    "                    new_xmaped[j] /= node_counts[j]\n",
    "            \n",
    "            xmaped = new_xmaped\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return xmaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a560cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ylabel.shape = (4230,)\n",
      "ylabel.shape = (19717,)\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "for dataset_name in ['CiteSeer', 'PubMed']:\n",
    "    xembed, eglist, ylabel, ylprob, xsvd = read_data(embedding_dimension=1,\n",
    "                                                     dataset_name=dataset_name, eps=1.0e-6)\n",
    "    print('ylabel.shape = ' + str(ylabel.shape))\n",
    "    nsample = xembed.shape[0]\n",
    "    idx_train, idx_ttest, idx_valid = perform_ttv_split(nsample, ftrain=0.6, fttest=0.2, fvalid=0.2)\n",
    "\n",
    "    datasets[dataset_name] = (xembed, ylabel, eglist, idx_train, idx_valid, idx_ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2275fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [3:27:52<00:00,  1.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4230/4230 [1:12:48<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 2538 samples\n",
      "Validation set: 846 samples\n",
      "Test set: 846 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4230/4230 [1:11:59<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 2538 samples\n",
      "Validation set: 846 samples\n",
      "Test set: 846 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4230/4230 [1:11:39<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 2538 samples\n",
      "Validation set: 846 samples\n",
      "Test set: 846 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [3:37:49<00:00,  1.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4230/4230 [58:32<00:00,  1.20it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 2538 samples\n",
      "Validation set: 846 samples\n",
      "Test set: 846 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [3:29:36<00:00,  1.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [1:26:08<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [1:12:50<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [1:12:44<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [1:11:59<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Train set: 11831 samples\n",
      "Validation set: 3943 samples\n",
      "Test set: 3943 samples\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"/home/ubuntu/simulations/nnet_folder\"):\n",
    "    model_path = f\"/home/ubuntu/simulations/nnet_folder/{filename}\"\n",
    "\n",
    "    model = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "    dataset_name = filename.split('_')[2]\n",
    "    xembed, ylabel, eglist, idx_train, idx_valid, idx_ttest = datasets[dataset_name]\n",
    "\n",
    "    extracted_features_result = extract_features_for_classifier(model, xembed, ylabel, eglist, \n",
    "                                        idx_train, idx_valid, idx_ttest, \n",
    "                                        nbatch=64)\n",
    "\n",
    "    folder_name = filename.split('.')[0]\n",
    "    os.makedirs(f\"/home/ubuntu/simulations/classificator_features/{folder_name}\", exist_ok=True)\n",
    "\n",
    "    for key, arr in extracted_features_result.items():\n",
    "        np.save(f\"/home/ubuntu/simulations/classificator_features/{folder_name}/{key}\", arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362dff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
